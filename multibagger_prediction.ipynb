{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìà Multibagger Stock Prediction Pipeline\n",
    "## Using EV √ó L Framework + Advanced ML Models\n",
    "\n",
    "**Goal**: Predict stocks with 2x+ returns in 3 years using fundamental analysis + machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "import xgboost as xgb\n",
    "from tensorflow import keras\n",
    "import shap\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 1: Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MultibaggerPredictor class\n",
    "from multibagger_prediction import MultibaggerPredictor\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = MultibaggerPredictor()\n",
    "\n",
    "# Load synthetic data\n",
    "df = predictor.load_data(generate_synthetic=True)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Multibagger ratio: {df['multibagger'].mean():.2%}\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Class distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "df['multibagger'].value_counts().plot(kind='bar')\n",
    "plt.title('Class Distribution: Multibagger vs Non-Multibagger')\n",
    "plt.xlabel('Multibagger (1=Yes, 0=No)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 2: EV √ó L Framework Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "df_clean = predictor.preprocess_data(df)\n",
    "print(f\"After preprocessing: {df_clean.shape}\")\n",
    "\n",
    "# Compute EV and L scores\n",
    "df_scored = predictor.compute_ev_l_scores(df_clean)\n",
    "\n",
    "# Display EV and L score distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# EV Score distribution\n",
    "axes[0].hist(df_scored['EV_Score'], bins=30, alpha=0.7, color='blue')\n",
    "axes[0].axvline(0.6, color='red', linestyle='--', label='Threshold (0.6)')\n",
    "axes[0].set_title('EV Score Distribution')\n",
    "axes[0].set_xlabel('EV Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "\n",
    "# L Score distribution\n",
    "axes[1].hist(df_scored['L_Score'], bins=30, alpha=0.7, color='green')\n",
    "axes[1].axvline(0.5, color='red', linestyle='--', label='Threshold (0.5)')\n",
    "axes[1].set_title('L Score Distribution')\n",
    "axes[1].set_xlabel('L Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EV vs L scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(df_scored['EV_Score'], df_scored['L_Score'], \n",
    "                     c=df_scored['multibagger'], cmap='viridis', alpha=0.6)\n",
    "plt.axvline(0.6, color='red', linestyle='--', alpha=0.7, label='EV Threshold')\n",
    "plt.axhline(0.5, color='red', linestyle='--', alpha=0.7, label='L Threshold')\n",
    "plt.xlabel('EV Score (Earnings Visibility)')\n",
    "plt.ylabel('L Score (Longevity)')\n",
    "plt.title('EV √ó L Framework: Stock Distribution')\n",
    "plt.colorbar(scatter, label='Multibagger (1=Yes, 0=No)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Filter candidates\n",
    "df_filtered = predictor.filter_candidates(df_scored)\n",
    "print(f\"\\nFiltered dataset shape: {df_filtered.shape}\")\n",
    "print(f\"Multibagger ratio in filtered data: {df_filtered['multibagger'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 3: Feature Engineering and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "X, y = predictor.prepare_features(df_filtered)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = X.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f')\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split (simulating chronological order)\n",
    "split_idx = int(0.7 * len(X))\n",
    "val_idx = int(0.85 * len(X))\n",
    "\n",
    "X_train, y_train = X.iloc[:split_idx], y.iloc[:split_idx]\n",
    "X_val, y_val = X.iloc[split_idx:val_idx], y.iloc[split_idx:val_idx]\n",
    "X_test, y_test = X.iloc[val_idx:], y.iloc[val_idx:]\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Scale features\n",
    "X_train_scaled = predictor.scaler.fit_transform(X_train)\n",
    "X_val_scaled = predictor.scaler.transform(X_val)\n",
    "X_test_scaled = predictor.scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n‚úÖ Data preparation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 4: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "models = predictor.train_models(X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "print(f\"\\n‚úÖ Trained {len(models)} models successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Step 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "results_df = predictor.evaluate_models(X_test_scaled, y_test)\n",
    "\n",
    "# Display results in a nice format\n",
    "print(\"\\nüìä Model Performance Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"{row['Model']:<20} | Accuracy: {row['Accuracy']:.3f} | Precision: {row['Precision']:.3f} | Recall: {row['Recall']:.3f} | F1: {row['F1']:.3f} | AUC: {row['AUC']:.3f}\")\n",
    "\n",
    "# Plot model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//2, i%2]\n",
    "    results_df.plot(x='Model', y=metric, kind='bar', ax=ax, color='skyblue')\n",
    "    ax.set_title(f'{metric} Comparison')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 6: Advanced Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive visualizations\n",
    "predictor.plot_results(df_filtered, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP analysis for feature importance\n",
    "if 'Random Forest' in predictor.models:\n",
    "    print(\"üîç SHAP Analysis for Random Forest Model:\")\n",
    "    \n",
    "    # Create SHAP explainer\n",
    "    rf_model = predictor.models['Random Forest']\n",
    "    explainer = shap.TreeExplainer(rf_model)\n",
    "    shap_values = explainer.shap_values(X_test_scaled[:100])  # Use first 100 samples\n",
    "    \n",
    "    # Feature importance plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values[1], X_test.iloc[:100], plot_type=\"bar\", show=False)\n",
    "    plt.title('SHAP Feature Importance (Random Forest)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values[1], X_test.iloc[:100], show=False)\n",
    "    plt.title('SHAP Summary Plot (Random Forest)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 7: Model Saving and Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "predictor.save_model('best_multibagger_model.pkl')\n",
    "\n",
    "# Test prediction on new data\n",
    "sample_stocks = [\n",
    "    {\n",
    "        'name': 'High Growth Tech Stock',\n",
    "        'PE_Ratio': 25.0,\n",
    "        'EPS': 12.5,\n",
    "        'ROE': 28.0,\n",
    "        'ROA': 15.2,\n",
    "        'Market_Cap': 10000000000,\n",
    "        'Revenue_Growth': 35.0,\n",
    "        'Profit_Margin': 22.5,\n",
    "        'Debt_Equity': 0.2,\n",
    "        'Volatility': 0.4,\n",
    "        'SectorGrowth': 18.0\n",
    "    },\n",
    "    {\n",
    "        'name': 'Stable Value Stock',\n",
    "        'PE_Ratio': 12.0,\n",
    "        'EPS': 8.0,\n",
    "        'ROE': 18.0,\n",
    "        'ROA': 10.5,\n",
    "        'Market_Cap': 5000000000,\n",
    "        'Revenue_Growth': 8.0,\n",
    "        'Profit_Margin': 12.0,\n",
    "        'Debt_Equity': 0.3,\n",
    "        'Volatility': 0.2,\n",
    "        'SectorGrowth': 6.0\n",
    "    },\n",
    "    {\n",
    "        'name': 'Risky Penny Stock',\n",
    "        'PE_Ratio': 45.0,\n",
    "        'EPS': 2.0,\n",
    "        'ROE': 8.0,\n",
    "        'ROA': 3.0,\n",
    "        'Market_Cap': 100000000,\n",
    "        'Revenue_Growth': 50.0,\n",
    "        'Profit_Margin': 5.0,\n",
    "        'Debt_Equity': 1.2,\n",
    "        'Volatility': 0.8,\n",
    "        'SectorGrowth': 25.0\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üîÆ Sample Predictions:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for stock in sample_stocks:\n",
    "    name = stock.pop('name')\n",
    "    probability = predictor.predict_new_stock(stock)\n",
    "    \n",
    "    print(f\"\\nüìä {name}:\")\n",
    "    print(f\"   Multibagger Probability: {probability:.1%}\")\n",
    "    \n",
    "    if probability >= 0.7:\n",
    "        print(f\"   Recommendation: üöÄ STRONG BUY\")\n",
    "    elif probability >= 0.5:\n",
    "        print(f\"   Recommendation: ‚ö° MODERATE BUY\")\n",
    "    else:\n",
    "        print(f\"   Recommendation: ‚ö†Ô∏è AVOID/HOLD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Step 8: Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ MULTIBAGGER PREDICTION PIPELINE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Total stocks analyzed: {len(df)}\")\n",
    "print(f\"üéØ Stocks passing EV√óL filter: {len(df_filtered)} ({len(df_filtered)/len(df)*100:.1f}%)\")\n",
    "print(f\"ü§ñ Models trained: {len(predictor.models)}\")\n",
    "print(f\"üèÜ Best model: {results_df.loc[results_df['F1'].idxmax(), 'Model']}\")\n",
    "print(f\"üìà Best F1 Score: {results_df['F1'].max():.3f}\")\n",
    "print(f\"üíæ Model saved: best_multibagger_model.pkl\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"1. Run Streamlit app: streamlit run streamlit_app.py\")\n",
    "print(\"2. Test with real stock data from Yahoo Finance\")\n",
    "print(\"3. Implement backtesting on historical data\")\n",
    "print(\"4. Add more sophisticated features (technical indicators, sentiment)\")\n",
    "print(\"5. Deploy to cloud platform for production use\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}